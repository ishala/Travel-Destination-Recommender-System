{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step By Step Pengerjaan (Deadline 3 September 2024)\n",
    "- Cocokin data byPlaceData.csv dengan scrapetable_wisata.xlsx\n",
    "- Mengubah konten menjadi bahasa indonesia, dengan mengambil data dari byPlaceData.csv\n",
    "- Data yang tidak ada di scrapetable_wisata.xlsx langsung di drop saja di byPlaceData.csv (mengurangi storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "DATA = 'data/'\n",
    "\n",
    "dataBaruPath = f'{DATA}scrapetable_wisata.xlsx'\n",
    "dataByPlace = f'{DATA}byPlaceData.csv'\n",
    "dataNewFix = f'{DATA}new_fixedData.xlsx'\n",
    "\n",
    "dfNew = pd.read_excel(dataBaruPath)\n",
    "dfByPlace = pd.read_csv(dataByPlace)\n",
    "dfNewFix = pd.read_excel(dataNewFix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNew.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNew.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfByPlace.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Assessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cek Info DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ByPlace\n",
    "print('By Place')\n",
    "print(dfByPlace.info())\n",
    "\n",
    "print()\n",
    "\n",
    "# new\n",
    "print('New')\n",
    "print(dfNew.info())\n",
    "\n",
    "print()\n",
    "\n",
    "# newfixed\n",
    "print('New Fixed')\n",
    "print(dfNewFix.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cek Nilai Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ByPlace\n",
    "print('By Place')\n",
    "print(dfByPlace.isna().sum())\n",
    "\n",
    "print()\n",
    "\n",
    "# new\n",
    "print('New')\n",
    "print(dfNew.isna().sum())\n",
    "\n",
    "print()\n",
    "\n",
    "# newfixed\n",
    "print('New Fixed')\n",
    "print(dfNewFix.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cek Nilai Duplikat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ByPlace\n",
    "print('By Place')\n",
    "print(dfByPlace.duplicated().sum())\n",
    "\n",
    "print()\n",
    "\n",
    "# new\n",
    "print('New')\n",
    "print(dfNew.duplicated().sum())\n",
    "\n",
    "print()\n",
    "\n",
    "# newfixed\n",
    "print('New Fixed')\n",
    "print(dfNewFix.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pemilihan Kolom Data Bu Melany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNew.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfNew.latitude.sample(1))\n",
    "print(dfNew.longitude.sample(1))\n",
    "\n",
    "print(dfByPlace.coordinates.sample(1))\n",
    "print(dfByPlace.address.sample(1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfNew.timezone.unique())\n",
    "print(dfNew.review_count.sample(1))\n",
    "print(dfNew.place_id.sample(1))\n",
    "print(dfNew.city.unique())\n",
    "print(dfNew['validasi di jogja dan tetangga'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 1: Menghapus kolom dengan banyak missing values\n",
    "dfByPlace_cleaned = dfByPlace.drop(columns=['is_rating_updated', 'is_reviews_updated'])\n",
    "\n",
    "# Dataset 2: Menghapus kolom dengan banyak missing values dan mengisi missing values lainnya\n",
    "dfNew_cleaned = dfNew.drop(columns=['price_level'])\n",
    "\n",
    "# Mengisi missing values pada kolom numerik dengan median\n",
    "dfNew_cleaned['phone_number'].fillna('Unknown', inplace=True)\n",
    "# dfNew_cleaned['review_count'].fillna(dfNew_cleaned['review_count'].median(), inplace=True) (Dipertimbangkan)\n",
    "dfNew_cleaned['rating'].fillna(dfNew_cleaned['rating'].median(), inplace=True)\n",
    "\n",
    "# Mengisi missing values pada kolom string dengan 'Unknown'\n",
    "dfNew_cleaned['website'].fillna('Unknown', inplace=True)\n",
    "columns_to_fill = ['Friday', 'Saturday', 'Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'state']\n",
    "for column in columns_to_fill:\n",
    "    dfNew_cleaned[column].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Memeriksa kembali missing values setelah pembersihan\n",
    "missing_values_dfByPlace_cleaned = dfByPlace_cleaned.isnull().sum()\n",
    "missing_values_dfNew_cleaned = dfNew_cleaned.isnull().sum()\n",
    "\n",
    "missing_values_dfByPlace_cleaned, missing_values_dfNew_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menggabungkan Data dan Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menggabungkan data dari kedua dataset berdasarkan kolom address\n",
    "# Menentukan keyword untuk filtering\n",
    "keywords = [\"Jawa Tengah\", \"Central Java\", \"Yogyakarta\", \"Special Region of Yogyakarta\"]\n",
    "\n",
    "# Filter Dataset 1\n",
    "filtered_dfByPlace = dfByPlace_cleaned[dfByPlace_cleaned['address'].str.contains('|'.join(keywords), case=False, na=False)]\n",
    "\n",
    "# Filter Dataset 2\n",
    "filtered_dfNew = dfNew_cleaned[dfNew_cleaned['full_address'].str.contains('|'.join(keywords), case=False, na=False)]\n",
    "\n",
    "# Menggabungkan kedua dataset yang telah difilter\n",
    "combined_dataset = pd.concat([filtered_dfByPlace, filtered_dfNew], ignore_index=True)\n",
    "\n",
    "# Menampilkan jumlah baris dan kolom dari dataset yang digabungkan\n",
    "combined_shape = combined_dataset.shape\n",
    "combined_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the names in both datasets to lowercase for better matching\n",
    "# dfByPlace['name'] = dfByPlace['name'].str.lower()\n",
    "# dfNew['name'] = dfNew['name']\n",
    "\n",
    "# Filter out rows in byPlaceData that do not have a matching name in scrapetable_wisata\n",
    "filtered_data = dfByPlace[(dfByPlace['name'].str.lower()).isin((dfNew['name']).str.lower())]\n",
    "\n",
    "# Now filter based on the address containing 'Jawa Tengah', 'Central Java', or 'Yogyakarta'\n",
    "filtered_data = filtered_data[\n",
    "    filtered_data['address'].str.contains('Jawa Tengah|Central Java|Yogyakarta', case=False)\n",
    "]\n",
    "\n",
    "# Display the first few rows of the filtered data to verify\n",
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the filtered data to a new CSV file\n",
    "# filtered_data_path = 'data/filtered_byPlaceData.csv'\n",
    "# filtered_data.to_csv(filtered_data_path, index=False)\n",
    "\n",
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambil Data dari DF Baru\n",
    "Pengambilan data yang mungkin masih merupakan wisata dari data baru, tapi tidak ada di data utama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpickedData = dfNew[~dfNew['name'].str.lower().isin(filtered_data['name'].str.lower())]\n",
    "\n",
    "filteredCols = filtered_data.columns\n",
    "unpickedCols = unpickedData.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data Fix\n",
    "\n",
    "Data yang disini sudah hasil pengambilan dari data baru, yang tidak ada di data lama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixedDf = pd.read_csv('data/fixed_data.csv')\n",
    "fixedDf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixedDf.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cek Nilai Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixedDf.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memindahkan Nilai dari Kolom ke Kolom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi drop kolom\n",
    "def dropCol(df, lsCol):\n",
    "    col1 = lsCol[0]\n",
    "    col2 = lsCol[1]\n",
    "    \n",
    "    if col1 == 'latitude' and col2 == 'longitude':\n",
    "        df.drop(columns=[col1, col2], inplace=True)\n",
    "    else:\n",
    "        df.drop(columns=[col2], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Fungsi Pemindah Nilai\n",
    "def moveValues(col1, col2, df):\n",
    "    # Kondisi coordinates\n",
    "    if col1 == 'latitude' and col2 == 'longitude':\n",
    "        # Menggabungkan koordinat dan menyimpannya pada kolom 'coordinates'\n",
    "        df['coordinates'] = df.apply(lambda row: f\"{row[col1]},{row[col2]}\" if pd.isna(row['coordinates']) else row['coordinates'], axis=1)\n",
    "        \n",
    "        # Menghapus kolom latitude dan longitude\n",
    "        df = dropCol(df, [col1, col2])\n",
    "    else:\n",
    "        # Mengecek baris dengan NaN pada col1\n",
    "        mask_nullCol1 = df[col1].isna()\n",
    "        \n",
    "        if mask_nullCol1.any():\n",
    "            df.loc[mask_nullCol1, col1] = df.loc[mask_nullCol1, col2]\n",
    "        else:\n",
    "            print('Data tidak ada isinya')\n",
    "        \n",
    "        # Menghapus kolom col1 dan col2\n",
    "        df = dropCol(df, [col1, col2])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kolom Address dan Full Address\n",
    "fixedDf = moveValues('address', 'full_address', fixedDf)\n",
    "# Kolom latitude dan longitude\n",
    "fixedDf = moveValues('latitude', 'longitude', fixedDf)\n",
    "# Kolom latitude dan longitude\n",
    "fixedDf = moveValues('reviews', 'review_count', fixedDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hapus Kolom Sisa yang Tidak Digunakan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "delCols = ['is_rating_updated', 'is_reviews_updated', 'website', \n",
    "           'business_id', 'phone_number', 'Unnamed: 0.1', 'Unnamed: 0', 'price_level']\n",
    "\n",
    "fixedDf.drop(columns=delCols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixedDf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menangani Nilai Null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. timezone (Faishal)\n",
    "   \n",
    "Diisi dengan nilai unique, yaitu Asia/Jakarta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixedDf['timezone'] = fixedDf.timezone.fillna('Asia/Jakarta', axis=0)\n",
    "print('Nilai unique: ', fixedDf.timezone.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. types (Akhdan)\n",
    "\n",
    "Diisi sesuai dengan bidangnya, dilihat dari nilai unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Nilai unique: ', fixedDf.types.unique())\n",
    "# fixedDf.timezone.fillna('Asia/Jakarta', axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. city (Faishal)\n",
    "   \n",
    "Diisi dengan menggunakan regex dari full address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk mengambil dua bagian terakhir setelah dipisahkan oleh koma\n",
    "def extractCity(df):\n",
    "    for i, row in df.iterrows():\n",
    "        address = row['address']\n",
    "        \n",
    "        # Periksa apakah kolom 'city' kosong (NaN)\n",
    "        if pd.isna(row['city']):\n",
    "            parts = address.split(',')\n",
    "            # Menggabungkan tiga bagian terakhir\n",
    "            df.at[i, 'city'] = ','.join(parts[-2:]).strip()\n",
    "    return df\n",
    "\n",
    "fixedDf = extractCity(fixedDf)\n",
    "\n",
    "# Karena removeNumber bekerja dengan Pandas Series, kita perlu menerapkannya pada keseluruhan kolom\n",
    "fixedDf['city'] = fixedDf['city'].apply(lambda x: pd.Series([x]).str.replace(r'\\d+', '', regex=True).item().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. workday_timing dan closed_on (Faishal)\n",
    "\n",
    "Ambil dari yang hari-hari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk mencari dan mengubah format waktu AM/PM yang tidak standar ke format 24 jam (contoh: '7:30 AM–6 PM' menjadi '07.30-18.00')\n",
    "def normalize_am_pm_format(time_str):\n",
    "    try:\n",
    "        # Deteksi format dengan AM/PM yang tidak standar, misalnya: '7:30 AM–6 PM'\n",
    "        time_ranges = re.findall(r'(\\d{1,2}:\\d{2}|\\d{1,2})(?:\\s?[AaPp][Mm])?', time_str)\n",
    "        if time_ranges and 'AM' in time_str or 'PM' in time_str:\n",
    "            converted_ranges = []\n",
    "            for i in range(0, len(time_ranges), 2):\n",
    "                start_time = time_ranges[i]\n",
    "                end_time = time_ranges[i + 1]\n",
    "                \n",
    "                # Konversi waktu awal dan akhir ke format 24 jam\n",
    "                start_time_24 = datetime.strptime(start_time, '%I:%M' if ':' in start_time else '%I').strftime('%H.%M')\n",
    "                end_time_24 = datetime.strptime(end_time, '%I:%M' if ':' in end_time else '%I').strftime('%H.%M')\n",
    "                \n",
    "                # Gabungkan rentang waktu yang sudah dikonversi\n",
    "                converted_ranges.append(f\"{start_time_24}-{end_time_24}\")\n",
    "            \n",
    "            return ', '.join(converted_ranges)  # Gabungkan semua rentang waktu yang terdeteksi\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    return time_str  # Jika tidak ada format AM/PM, kembalikan string aslinya\n",
    "\n",
    "# Regex untuk standarisasi rentang waktu (misalnya: konversi rentang waktu seperti 5.00-18.00 menjadi 05.00-18.00)\n",
    "def standardize_time_range(time_str):\n",
    "    # Pattern untuk mendeteksi format waktu (misalnya 5:30-18:00 atau 6.00-23.00)\n",
    "    pattern = r'(\\d{1,2})[:\\.](\\d{2})-(\\d{1,2})[:\\.](\\d{2})'\n",
    "    match = re.match(pattern, time_str)\n",
    "    if match:\n",
    "        return f\"{match.group(1).zfill(2)}.{match.group(2)}-{match.group(3).zfill(2)}.{match.group(4)}\"\n",
    "    return time_str\n",
    "\n",
    "# Workday Timing\n",
    "def fillWorkdayTime(df, lsDays):\n",
    "    for i, row in df.iterrows():\n",
    "        tempTime = []\n",
    "        \n",
    "        for col in lsDays:\n",
    "            if not pd.isna(row[col]):\n",
    "                tempTime.append(row[col])\n",
    "                \n",
    "        if len(tempTime) > 0:\n",
    "            # Menghitung frekuensi kemunculan dan mengambil yang paling sering\n",
    "            most_common_time = Counter(tempTime).most_common(1)[0][0]\n",
    "            \n",
    "            # Terapkan normalisasi format AM/PM dan standarisasi waktu\n",
    "            most_common_time = normalize_am_pm_format(most_common_time)\n",
    "            most_common_time = standardize_time_range(most_common_time)\n",
    "            \n",
    "            # Simpan hasil ke kolom 'workday_timing'\n",
    "            df.at[i, 'workday_timing'] = most_common_time\n",
    "        else:\n",
    "            df.at[i, 'workday_timing'] = 'Not Present'  # Jika semua nilai NaN, beri nilai default\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Closed On\n",
    "# Fungsi untuk mengisi hari-hari yang \"Closed On\"\n",
    "def fillClosedOn(df, lsDays, day_translation):\n",
    "    # Urutan hari dari Senin sampai Minggu\n",
    "    order_days = ['Senin', 'Selasa', 'Rabu', 'Kamis', 'Jumat', 'Sabtu', 'Minggu']\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        tempWord = row['closed_on']  # Mulai dengan nilai existing di kolom 'closed_on'\n",
    "        \n",
    "        # Jika closed_on sudah ada isinya, lewati baris ini\n",
    "        if tempWord and not pd.isna(tempWord):\n",
    "            continue\n",
    "        \n",
    "        closed_days = []  # List untuk menyimpan hari-hari yang tutup berdasarkan lsDays\n",
    "        all_nan = True    # Flag untuk mengecek apakah semua kolom lsDays adalah NaN\n",
    "        \n",
    "        # Iterasi melalui hari-hari dalam lsDays\n",
    "        for col in lsDays:\n",
    "            if pd.isna(row[col]):\n",
    "                closed_days.append(day_translation[col])  # Catat hari yang tutup\n",
    "            else:\n",
    "                all_nan = False  # Jika ada yang bukan NaN, berarti tidak semua hari tutup\n",
    "\n",
    "        # Jika semua kolom hari adalah NaN, isi 'Open All Days'\n",
    "        if all_nan:\n",
    "            tempWord = 'Open All Days'\n",
    "        else:\n",
    "            if closed_days:  # Jika ada hari yang tutup\n",
    "                tempWord = ', '.join(closed_days)\n",
    "                # Menghilangkan hari yang berulang dan mengurutkan\n",
    "                unique_days = sorted(set(tempWord.split(', ')), key=order_days.index)\n",
    "                tempWord = ', '.join(unique_days)\n",
    "        \n",
    "        # Simpan hasil akhir\n",
    "        df.at[i, 'closed_on'] = tempWord\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List Days\n",
    "lsDays = ['Sunday', 'Monday', 'Tuesday', 'Wednesday',\n",
    "          'Thursday', 'Friday', 'Saturday']\n",
    "\n",
    "# Dictionary untuk menerjemahkan hari dari bahasa Inggris ke bahasa Indonesia\n",
    "dayTranslation = {\n",
    "    'Sunday': 'Minggu',\n",
    "    'Monday': 'Senin',\n",
    "    'Tuesday': 'Selasa',\n",
    "    'Wednesday': 'Rabu',\n",
    "    'Thursday': 'Kamis',\n",
    "    'Friday': 'Jumat',\n",
    "    'Saturday': 'Sabtu'\n",
    "}\n",
    "\n",
    "# Panggil fungsi fillWorkdayTime\n",
    "fixedDf = fillWorkdayTime(fixedDf, lsDays=lsDays)\n",
    "\n",
    "# Panggil fungsi fillClosedOn\n",
    "fixedDf = fillClosedOn(fixedDf, lsDays, dayTranslation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Kolom-kolom Hari\n",
    "fixedDf = fixedDf.drop(lsDays, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. reviews dan rating (Akhdan)\n",
    "   \n",
    "Manual aja ambil dari maps, kalau memungkinkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil data dari isian manual\n",
    "manualDf = pd.read_excel(f'{DATA}new_fixedData.xlsx')\n",
    "\n",
    "# Merge the datasets on the 'name' column\n",
    "merged_df = pd.merge(fixedDf, manualDf[['name', 'reviews', 'rating']], on='name', how='left', suffixes=('', '_manual'))\n",
    "\n",
    "# Update null values in 'reviews' and 'rating' columns in fixeddf with values from manualdf\n",
    "merged_df['reviews'] = merged_df['reviews'].combine_first(merged_df['reviews_manual'])\n",
    "merged_df['rating'] = merged_df['rating'].combine_first(merged_df['rating_manual'])\n",
    "\n",
    "# Drop the manual columns used for the update\n",
    "fixedDf = merged_df.drop(columns=['reviews_manual', 'rating_manual'])\n",
    "\n",
    "# fill null with '0'\n",
    "fixedDf.reviews.fillna(0, inplace=True)\n",
    "fixedDf.rating.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. reviewer_name, rating_review, dan review_text (Akhdan)\n",
    "\n",
    "ambil dari maps juga, masing2 data 1 ajaa kalo yang null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfDataReviewer(df1, df2):\n",
    "    cols = ['reviewer_name', 'rating_review', 'review_text']\n",
    "    \n",
    "    # Melakukan merge untuk mendapatkan nilai dari df2 ke df1 berdasarkan kolom 'name'\n",
    "    merged_df = df1.merge(df2, left_on='name', right_on='name', suffixes=('', '_manual'))\n",
    "    \n",
    "    # Mengisi nilai null di df1 dengan nilai dari df2\n",
    "    for col in cols:\n",
    "        df1[col] = merged_df[col].combine_first(merged_df[col + '_manual'])\n",
    "    \n",
    "    # Menghapus kolom-kolom dengan suffix '_manual'\n",
    "    cols_to_drop = [col + '_manual' for col in cols]\n",
    "    df1 = df1.drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    return df1\n",
    "\n",
    "fixedDf = tfDataReviewer(fixedDf, manualDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. workday_timing & closed_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixedDf['workday_timing'] = fixedDf['workday_timing'].fillna('Not Present', axis=0)\n",
    "fixedDf['closed_on'] = fixedDf['closed_on'].fillna('Not Present', axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. most_popular_times & popular_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixedDf['most_popular_times'] = fixedDf['most_popular_times'].fillna('Not Present', axis=0)\n",
    "fixedDf['popular_times'] = fixedDf['popular_times'].fillna('Not Present', axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. validasi di jogja dan tetangga (Faishal)\n",
    "\n",
    "Langsung isi pake nilai terbanyak aja, keknya udah di jogja dan sekitarnya semua ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixedDf['validasi di jogja dan tetangga'] = fixedDf['validasi di jogja dan tetangga'].fillna('YA', axis=0)\n",
    "fixedDf['validasi di jogja dan tetangga'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. accessibility_enabled dan children_enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fixedDf.accessibility_enabled.unique())\n",
    "print(fixedDf.children_enabled.unique())\n",
    "\n",
    "fixedDf['accessibility_enabled'] = fixedDf.accessibility_enabled.fillna('Informasi Tidak Tersedia')\n",
    "fixedDf['children_enabled'] = fixedDf.children_enabled.fillna('Informasi Tidak Tersedia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menghapus Kolom yang Tidak Diperlukan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixedDf = fixedDf.drop(['timezone', 'place_id', 'place_link',\n",
    "                        'verified', 'state', \n",
    "                        'validasi di jogja dan tetangga',\n",
    "                        'published_at_date'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate Ke Bahasa Indonesia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ganti bahasa indonesia\n",
    "translationDict = {\n",
    "    'Bantul Regency, Special Region of Yogyakarta': 'Kabupaten Bantul, Daerah Istimewa Yogyakarta',\n",
    "    'Bantul Regency, Special Region of Yogyakarta, Indonesia': 'Kabupaten Bantul, Daerah Istimewa Yogyakarta',\n",
    "    'Yogyakarta City, Special Region of Yogyakarta': 'Kota Yogyakarta, Daerah Istimewa Yogyakarta',\n",
    "    'Sleman Regency, Special Region of Yogyakarta': 'Kabupaten Sleman, Daerah Istimewa Yogyakarta',\n",
    "    'Magelang Regency, Central Java': 'Kabupaten Magelang, Jawa Tengah',\n",
    "    'Magelang Regency, Central Java, Indonesia': 'Kabupaten Magelang, Jawa Tengah',\n",
    "    'Kulon Progo Regency, Special Region of Yogyakarta, Indonesia': 'Kabupaten Kulon Progo, Daerah Istimewa Yogyakarta',\n",
    "    'yogyakarta, Special Region of Yogyakarta, Indonesia': 'Kota Yogyakarta, Daerah Istimewa Yogyakarta',\n",
    "    'Yogyakarta City, Special Region of Yogyakarta, Indonesia': 'Kota Yogyakarta, Daerah Istimewa Yogyakarta',\n",
    "    'Klaten Regency, Central Java, Indonesia': 'Kabupaten Klaten, Jawa Tengah',\n",
    "    'Sleman Regency, Special Region of Yogyakarta': 'Kabupaten Sleman, Daerah Istimewa Yogyakarta',\n",
    "    'Sleman Regency, Special Region of Yogyakarta, Indonesia': 'Kabupaten Sleman, Daerah Istimewa Yogyakarta',\n",
    "    'Bantul Regency, Jawa Tengah, Indonesia': 'Kabupaten Bantul, Daerah Istimewa Yogyakarta',\n",
    "    'Gunung Kidul Regency, Special Region of Yogyakarta, Indonesia': 'Kabupaten Gunung Kidul, Daerah istimewa Yogyakarta',\n",
    "    'Purworejo Regency, Central Java, Indonesia': 'Kabupaten Purworejo, Jawa Tengah'\n",
    "}\n",
    "\n",
    "# Fungsi untuk melakukan translasi berdasarkan dictionary\n",
    "def translate_city(city):\n",
    "    return translationDict.get(city, city)\n",
    "\n",
    "# Apply fungsi translasi ke kolom city\n",
    "fixedDf['city'] = fixedDf['city'].apply(translate_city)\n",
    "fixedDf.city.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### workday_timing & closed_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list closed\n",
    "# closed_list = ['Open All Days']\n",
    "\n",
    "# ganti bahasa indonesia\n",
    "Time_translationDict = {\n",
    "    'Open All Days': 'Buka Setiap Hari',\n",
    "    'Not Present': 'Tidak Tersedia'\n",
    "}\n",
    "\n",
    "# Fungsi untuk melakukan translasi berdasarkan dictionary\n",
    "def translate_closed_on(closed_on):\n",
    "    return Time_translationDict.get(closed_on, closed_on)\n",
    "\n",
    "# Apply fungsi translasi ke kolom closed_on\n",
    "fixedDf['closed_on'] = fixedDf['closed_on'].apply(translate_closed_on)\n",
    "fixedDf.closed_on.unique()\n",
    "\n",
    "# Panggil fungsi fillClosedOn\n",
    "# fixedDf = fillClosedOn(fixedDf, closed_list, Time_translationDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixedDf.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### most_popular_times & popular_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk mengganti \"Time Label\" dan \"Average Popularity\" ke bahasa Indonesia\n",
    "def translateToIndo(text):\n",
    "    translations = {\n",
    "        'Time Label': 'Label Waktu',\n",
    "        'Average Popularity': 'Rata-rata Popularitas',\n",
    "        'Not Present': 'Tidak Tersedia',\n",
    "        'Sunday': 'Minggu',\n",
    "        'Monday': 'Senin',\n",
    "        'Tuesday': 'Selasa',\n",
    "        'Wednesday': 'Rabu',\n",
    "        'Thursday': 'Kamis',\n",
    "        'Friday': 'Jumat',\n",
    "        'Saturday': 'Sabtu',\n",
    "        'Idle': 'Diam'\n",
    "    }\n",
    "    \n",
    "    # Pastikan text adalah string sebelum memulai penggantian\n",
    "    if isinstance(text, str):\n",
    "        for eng, indo in translations.items():\n",
    "            text = text.replace(eng, indo)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Terapkan fungsi ke kolom most_popular_times\n",
    "fixedDf['most_popular_times'] = fixedDf['most_popular_times'].apply(translateToIndo)\n",
    "fixedDf['popular_times'] = fixedDf['popular_times'].apply(translateToIndo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixedDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixedDf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Kolom hidden_gem dan planning_enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghapus kolom 'hidden_gem' dan 'planning_enabled'\n",
    "fixedDf = fixedDf.drop(columns=['Hidden_gem', 'planning_enabled'])\n",
    "\n",
    "# Menampilkan hasil setelah penghapusan kolom\n",
    "print(fixedDf.info())\n",
    "print()\n",
    "fixedDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cek Korelasi Kolom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Function to convert workday_timing to duration in hours (e.g., \"07.00-21.00\")\n",
    "# def convert_to_hours(timing):\n",
    "#     if pd.isna(timing) or \"Buka\" in timing:  # Handle missing or \"Buka 24 jam\"\n",
    "#         return 24\n",
    "#     try:\n",
    "#         start, end = timing.split('-')\n",
    "#         start_hours = int(start.split('.')[0]) + int(start.split('.')[1]) / 60\n",
    "#         end_hours = int(end.split('.')[0]) + int(end.split('.')[1]) / 60\n",
    "#         return (end_hours - start_hours) if end_hours > start_hours else (24 - start_hours + end_hours)\n",
    "#     except:\n",
    "#         return None\n",
    "\n",
    "# # Apply conversion\n",
    "# fixedDf['workday_hours'] = fixedDf['workday_timing'].apply(convert_to_hours)\n",
    "\n",
    "# # Convert closed_on to binary: 1 for \"Buka Setiap Hari\", 0 otherwise\n",
    "# fixedDf['closed_on_binary'] = fixedDf['closed_on'].apply(lambda x: 1 if x == \"Buka Setiap Hari\" else 0)\n",
    "\n",
    "# # Ensure reviews and rating are numeric\n",
    "# fixedDf['reviews'] = pd.to_numeric(fixedDf['reviews'], errors='coerce')\n",
    "# fixedDf['rating'] = pd.to_numeric(fixedDf['rating'], errors='coerce')\n",
    "\n",
    "# # Compute the correlation matrix for the relevant columns\n",
    "# correlation_columns = ['workday_hours', 'closed_on_binary', 'reviews', 'rating']\n",
    "# correlation_matrix = fixedDf[correlation_columns].corr()\n",
    "\n",
    "# # Display the correlation matrix\n",
    "# print(correlation_matrix)\n",
    "\n",
    "# # Plot heatmap of the correlation matrix\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", linewidths=0.5)\n",
    "# plt.title(\"Correlation Matrix Heatmap\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to convert workday_timing to duration in hours (e.g., \"07.00-21.00\")\n",
    "# def convert_to_hours(timing):\n",
    "#     if pd.isna(timing) or \"Buka\" in timing:  # Handle missing or \"Buka 24 jam\"\n",
    "#         return 24\n",
    "#     try:\n",
    "#         start, end = timing.split('-')\n",
    "#         start_hours = int(start.split('.')[0]) + int(start.split('.')[1]) / 60\n",
    "#         end_hours = int(end.split('.')[0]) + int(end.split('.')[1]) / 60\n",
    "#         return (end_hours - start_hours) if end_hours > start_hours else (24 - start_hours + end_hours)\n",
    "#     except:\n",
    "#         return None\n",
    "\n",
    "# # Apply conversion\n",
    "# fixedDf['workday_hours'] = fixedDf['workday_timing'].apply(convert_to_hours)\n",
    "\n",
    "# # Convert closed_on to binary: 1 for \"Buka Setiap Hari\", 0 otherwise\n",
    "# fixedDf['closed_on_binary'] = fixedDf['closed_on'].apply(lambda x: 1 if x == \"Buka Setiap Hari\" else 0)\n",
    "\n",
    "# # Ensure reviews and rating are numeric\n",
    "# fixedDf['reviews'] = pd.to_numeric(fixedDf['reviews'], errors='coerce')\n",
    "# fixedDf['rating'] = pd.to_numeric(fixedDf['rating'], errors='coerce')\n",
    "\n",
    "# # Step 1: One-hot encode the 'types' column\n",
    "# types_encoded = pd.get_dummies(fixedDf['types'], prefix='type')\n",
    "\n",
    "# # Step 2: Combine the numeric columns with the one-hot encoded types\n",
    "# data_combined = pd.concat([fixedDf[['workday_hours', 'closed_on_binary', 'reviews']], types_encoded], axis=1)\n",
    "\n",
    "# # Step 3: Calculate the correlation matrix\n",
    "# correlation_matrix = data_combined.corr()\n",
    "\n",
    "# # Step 4: Extract correlations between numeric columns and the one-hot encoded types\n",
    "# correlation_with_types = correlation_matrix.loc[['workday_hours', 'closed_on_binary', 'reviews'], types_encoded.columns]\n",
    "# correlation_with_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixedDf.to_csv('data/fixedData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengatur gaya visualisasi\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 1. Memeriksa missing values\n",
    "missing_values_dfByPlace = dfByPlace.isnull().sum()\n",
    "missing_values_dfNew = dfNew.isnull().sum()\n",
    "missing_values_dfNewFix = dfNewFix.isnull().sum()\n",
    "\n",
    "# 2. Visualisasi distribusi rating\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Dataset 1\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(dfByPlace['rating'], bins=20, kde=True)\n",
    "plt.title('Distribusi Rating - Dataset 1')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Frekuensi')\n",
    "\n",
    "# Dataset 2\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(dfNew['rating'], bins=20, kde=True)\n",
    "plt.title('Distribusi Rating - Dataset 2')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Frekuensi')\n",
    "\n",
    "# Dataset 3\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(dfNewFix['rating'], bins=20, kde=True)\n",
    "plt.title('Distribusi Rating - Dataset 2')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Frekuensi')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Visualisasi distribusi jumlah review\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Dataset 1\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(dfByPlace['reviews'], bins=20, kde=True)\n",
    "plt.title('Distribusi Jumlah Review - Dataset 1')\n",
    "plt.xlabel('Jumlah Review')\n",
    "plt.ylabel('Frekuensi')\n",
    "\n",
    "# Dataset 2\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(dfNew['review_count'], bins=20, kde=True)\n",
    "plt.title('Distribusi Jumlah Review - Dataset 2')\n",
    "plt.xlabel('Jumlah Review')\n",
    "plt.ylabel('Frekuensi')\n",
    "\n",
    "# Dataset 3\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(dfNewFix['review_count'], bins=20, kde=True)\n",
    "plt.title('Distribusi Jumlah Review - Dataset 2')\n",
    "plt.xlabel('Jumlah Review')\n",
    "plt.ylabel('Frekuensi')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "missing_values_dfByPlace, missing_values_dfNew, missing_values_dfNewFix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
